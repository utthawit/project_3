{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f945e35-698e-4c02-9db3-af9a5208a7ab",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106b514e-5683-4faf-8f4c-675e07f15833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a1085-b3ff-466f-a2b9-2d6dcf7dd106",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a147786e-9227-4dc8-89ae-2a3b1f04ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df = pd.read_csv('../data/subreddit_combine_title_body.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a73ea4-08cb-4f2e-84b5-259c7ea97199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17j7oej</td>\n",
       "      <td>https://www.reddit.com/r/wine/comments/17j7oej...</td>\n",
       "      <td>743</td>\n",
       "      <td>2023-10-30 00:18:37</td>\n",
       "      <td>wine</td>\n",
       "      <td>[Megathread] How much is my wine worth? Is it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1gmbv5t</td>\n",
       "      <td>https://www.reddit.com/r/wine/comments/1gmbv5t...</td>\n",
       "      <td>16</td>\n",
       "      <td>2024-11-08 13:00:27</td>\n",
       "      <td>wine</td>\n",
       "      <td>Free Talk Friday Bottle porn without notes, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1gogepp</td>\n",
       "      <td>https://i.redd.it/6gdvjahxb60e1.jpeg</td>\n",
       "      <td>21</td>\n",
       "      <td>2024-11-11 08:19:05</td>\n",
       "      <td>wine</td>\n",
       "      <td>Started Journey to Master I have great study m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1goebub</td>\n",
       "      <td>https://www.reddit.com/gallery/1goebub</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-11-11 06:36:49</td>\n",
       "      <td>wine</td>\n",
       "      <td>NV Pierre Peters Howdy Winos! Anyone have any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1goj0bf</td>\n",
       "      <td>https://www.reddit.com/r/wine/comments/1goj0bf...</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-11-11 10:37:00</td>\n",
       "      <td>wine</td>\n",
       "      <td>Vouvray Chenin Blanc I mostly drink reds, but ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                url comms_num  \\\n",
       "0  17j7oej  https://www.reddit.com/r/wine/comments/17j7oej...       743   \n",
       "1  1gmbv5t  https://www.reddit.com/r/wine/comments/1gmbv5t...        16   \n",
       "2  1gogepp               https://i.redd.it/6gdvjahxb60e1.jpeg        21   \n",
       "3  1goebub             https://www.reddit.com/gallery/1goebub         7   \n",
       "4  1goj0bf  https://www.reddit.com/r/wine/comments/1goj0bf...        13   \n",
       "\n",
       "               created subreddit  \\\n",
       "0  2023-10-30 00:18:37      wine   \n",
       "1  2024-11-08 13:00:27      wine   \n",
       "2  2024-11-11 08:19:05      wine   \n",
       "3  2024-11-11 06:36:49      wine   \n",
       "4  2024-11-11 10:37:00      wine   \n",
       "\n",
       "                                          title_body  \n",
       "0  [Megathread] How much is my wine worth? Is it ...  \n",
       "1  Free Talk Friday Bottle porn without notes, ra...  \n",
       "2  Started Journey to Master I have great study m...  \n",
       "3  NV Pierre Peters Howdy Winos! Anyone have any ...  \n",
       "4  Vouvray Chenin Blanc I mostly drink reds, but ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb67ebc-f9aa-4085-97e6-156bc4c6b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word beer or wine from posts data\n",
    "combine_df['title_body'] = combine_df['title_body'].str.replace(r'beer|wine','', regex=True, case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd60e7-8a84-40bc-86d9-a0d9c275427e",
   "metadata": {},
   "source": [
    "#### Vertorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ab6039-252d-482b-adb2-db2ad3ba5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined base word helper function\n",
    "lemma_token = WordNetLemmatizer()\n",
    "stem_token = PorterStemmer()\n",
    "\n",
    "def lemma_tokenizer(words):\n",
    "    return ' '.join([lemma_token.lemmatize(w) for w in words.split()])\n",
    "\n",
    "def stem_tokenizer(words):\n",
    "    return ' '.join([stem_token.stem(w) for w in words.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e4a4dc-ad4b-42b7-9fb1-bdf0ade28513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup features\n",
    "X = combine_df['title_body']\n",
    "y = combine_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b53f6d-3650-4706-bbc2-cd13510f0657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "wine    53.16\n",
       "beer    46.84\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find baseline\n",
    "y.value_counts(normalize=True).mul(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186c57c5-ba40-46c5-b0b8-3cebb7352279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split with Lemmatizer\n",
    "X_lemma = X.apply(lemma_tokenizer)\n",
    "X_lemma_train, X_lemma_test, y_lemma_train, y_lemma_test = \\\n",
    "train_test_split(X_lemma, y, stratify=y, random_state=42, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d4d033-02fb-4dfe-831a-9b5d5e4f213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split with Porter stemmer\n",
    "X_stem = X.apply(stem_tokenizer)\n",
    "X_stem_train, X_stem_test, y_stem_train, y_stem_test = \\\n",
    "train_test_split(X_stem, y, stratify=y, random_state=42, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607da808-2b89-47f6-90cf-02e999067f83",
   "metadata": {},
   "source": [
    "#### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b73e09a5-e318-48c2-b34c-5faba71ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined models and parameters\n",
    "\n",
    "vectors = {\n",
    "            'CountVectorizer': {'pipeline': ('vec', CountVectorizer()),\n",
    "                                'grid_params': {'vec__stop_words': [None, 'english'],\n",
    "                                           'vec__min_df': [2, 3, 5],\n",
    "                                           'vec__ngram_range': [(1,1), (1,2)]\n",
    "                                          }\n",
    "                               },\n",
    "            'TfidfVectorizer': {'pipeline': ('vec', TfidfVectorizer()),\n",
    "                                'grid_params': {'vec__stop_words': [None, 'english'],\n",
    "                                           'vec__min_df': [2, 3, 5],\n",
    "                                           'vec__ngram_range': [(1,1), (1,2)]\n",
    "                                          }\n",
    "                               },\n",
    "}\n",
    "\n",
    "models = {'Naive Bayes': {'pipeline': ('nb',MultinomialNB()),\n",
    "                          'grid_params': {\n",
    "                                          'nb__alpha': [0.001, 0.05, 0.1, 1]\n",
    "                                         }\n",
    "        },\n",
    "        'Logistic Regression': \n",
    "                          {'pipeline': ('lr',LogisticRegression()),\n",
    "                           'grid_params': {\n",
    "                                          'lr__C': [0.001, 0.05, 0.1, 1],\n",
    "                                          'lr__solver': ['liblinear']\n",
    "                                         }\n",
    "        },\n",
    "        'RandomForest Classifier': \n",
    "                          {'pipeline': ('rfc', RandomForestClassifier()),\n",
    "                          'grid_params': {\n",
    "                                          'rfc__min_samples_split': [2 ,4, 8, 10],\n",
    "                                          'rfc__min_samples_leaf': [2 ,4, 8, 10],\n",
    "                                          'rfc__max_depth': [None, 1, 5 ,10, 15]\n",
    "                                         }\n",
    "        },\n",
    "}\n",
    "\n",
    "datas = {\n",
    "    'WordNetLemmatizer': {'X_train': X_lemma_train, 'X_test': X_lemma_test, \n",
    "                          'y_train': y_lemma_train, 'y_test': y_lemma_test},\n",
    "    'PorterStemmer' :    {'X_train': X_stem_train, 'X_test': X_stem_test, \n",
    "                          'y_train': y_stem_train, 'y_test': y_stem_test}    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d68ad1e2-2797-4c72-af6f-9ddc1723bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pipeline and gridsearch params\n",
    "vec_models = {}\n",
    "for vec_name, vector in vectors.items():\n",
    "    for cls_name, model in models.items():\n",
    "        vec_models[(vec_name, cls_name)] = {\n",
    "            'pipeline':   Pipeline([\n",
    "                vector['pipeline'],\n",
    "                model['pipeline'],\n",
    "            ]),\n",
    "            'grid_params': {\n",
    "                **vector['grid_params'],\n",
    "                **model['grid_params'],\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5753ef-0c44-4987-a087-0f4973f36e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Running gridsearch for (CountVectorizer, Naive Bayes, WordNetLemmatizer)\n",
      "Best parameters is : {'nb__alpha': 0.1, 'vec__min_df': 2, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}\n",
      "Best cross validation score is : 0.90354733\n",
      "Best training score is : 0.97137523\n",
      "Best testing score is : 0.90298507\n",
      "--------------------------------------------------------------------------------\n",
      "2. Running gridsearch for (CountVectorizer, Logistic Regression, WordNetLemmatizer)\n",
      "Best parameters is : {'lr__C': 0.1, 'lr__solver': 'liblinear', 'vec__min_df': 2, 'vec__ngram_range': (1, 1), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.87617021\n",
      "Best training score is : 0.98133167\n",
      "Best testing score is : 0.89054726\n",
      "--------------------------------------------------------------------------------\n",
      "3. Running gridsearch for (CountVectorizer, RandomForest Classifier, WordNetLemmatizer)\n",
      "Best parameters is : {'rfc__max_depth': 15, 'rfc__min_samples_leaf': 2, 'rfc__min_samples_split': 4, 'vec__min_df': 2, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.87678358\n",
      "Best training score is : 0.92221531\n",
      "Best testing score is : 0.86815920\n",
      "--------------------------------------------------------------------------------\n",
      "4. Running gridsearch for (TfidfVectorizer, Naive Bayes, WordNetLemmatizer)\n",
      "Best parameters is : {'nb__alpha': 0.1, 'vec__min_df': 2, 'vec__ngram_range': (1, 1), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.89981521\n",
      "Best training score is : 0.98755445\n",
      "Best testing score is : 0.91044776\n",
      "--------------------------------------------------------------------------------\n",
      "5. Running gridsearch for (TfidfVectorizer, Logistic Regression, WordNetLemmatizer)\n",
      "Best parameters is : {'lr__C': 1, 'lr__solver': 'liblinear', 'vec__min_df': 2, 'vec__ngram_range': (1, 1), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.89048066\n",
      "Best training score is : 0.98630989\n",
      "Best testing score is : 0.91044776\n",
      "--------------------------------------------------------------------------------\n",
      "6. Running gridsearch for (TfidfVectorizer, RandomForest Classifier, WordNetLemmatizer)\n",
      "Best parameters is : {'rfc__max_depth': 10, 'rfc__min_samples_leaf': 4, 'rfc__min_samples_split': 8, 'vec__min_df': 2, 'vec__ngram_range': (1, 1), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.87243732\n",
      "Best training score is : 0.91039203\n",
      "Best testing score is : 0.87064677\n",
      "--------------------------------------------------------------------------------\n",
      "7. Running gridsearch for (CountVectorizer, Naive Bayes, PorterStemmer)\n",
      "Best parameters is : {'nb__alpha': 0.1, 'vec__min_df': 2, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.89607768\n",
      "Best training score is : 0.98195395\n",
      "Best testing score is : 0.90547264\n",
      "--------------------------------------------------------------------------------\n",
      "8. Running gridsearch for (CountVectorizer, Logistic Regression, PorterStemmer)\n",
      "Best parameters is : {'lr__C': 0.1, 'lr__solver': 'liblinear', 'vec__min_df': 2, 'vec__ngram_range': (1, 1), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.87118502\n",
      "Best training score is : 0.98070940\n",
      "Best testing score is : 0.87313433\n",
      "--------------------------------------------------------------------------------\n",
      "9. Running gridsearch for (CountVectorizer, RandomForest Classifier, PorterStemmer)\n",
      "Best parameters is : {'rfc__max_depth': None, 'rfc__min_samples_leaf': 4, 'rfc__min_samples_split': 10, 'vec__min_df': 2, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.87553825\n",
      "Best training score is : 0.93777225\n",
      "Best testing score is : 0.85820896\n",
      "--------------------------------------------------------------------------------\n",
      "10. Running gridsearch for (TfidfVectorizer, Naive Bayes, PorterStemmer)\n",
      "Best parameters is : {'nb__alpha': 1, 'vec__min_df': 2, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.89296977\n",
      "Best training score is : 0.98942128\n",
      "Best testing score is : 0.91791045\n",
      "--------------------------------------------------------------------------------\n",
      "11. Running gridsearch for (TfidfVectorizer, Logistic Regression, PorterStemmer)\n",
      "Best parameters is : {'lr__C': 1, 'lr__solver': 'liblinear', 'vec__min_df': 3, 'vec__ngram_range': (1, 1), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.88985645\n",
      "Best training score is : 0.97448662\n",
      "Best testing score is : 0.89800995\n",
      "--------------------------------------------------------------------------------\n",
      "12. Running gridsearch for (TfidfVectorizer, RandomForest Classifier, PorterStemmer)\n",
      "Best parameters is : {'rfc__max_depth': None, 'rfc__min_samples_leaf': 2, 'rfc__min_samples_split': 10, 'vec__min_df': 2, 'vec__ngram_range': (1, 1), 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.86247390\n",
      "Best training score is : 0.98568762\n",
      "Best testing score is : 0.84825871\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "out_models = {}\n",
    "i = 1\n",
    "# initialize DataFrame to store score output \n",
    "model_df = pd.DataFrame(columns=['Word_Normalizer', 'Vertorizer_name', 'Model_Name', 'CV_Score', 'Training_Score', 'Testing_Score'])\n",
    "for group_name, data in datas.items():\n",
    "    X_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n",
    "    for name, model in vec_models.items():\n",
    "        vec_name, cls_name = name\n",
    "        key = (vec_name, cls_name, group_name)\n",
    "        print(f\"{i}. Running gridsearch for ({vec_name}, {cls_name}, {group_name})\")\n",
    "        out_models[key] = GridSearchCV(estimator=model['pipeline'], param_grid=model['grid_params'], \\\n",
    "                            scoring='accuracy', n_jobs=10, cv=2)\n",
    "        out_models[key].fit(X_train, y_train)\n",
    "        \n",
    "        # add store for model\n",
    "        model_df.loc[i] = [group_name, vec_name, cls_name, out_models[key].best_score_, \n",
    "                            out_models[key].score(X_train, y_train), out_models[key].score(X_test, y_test)]\n",
    "        \n",
    "        # display model result\n",
    "        print(f\"Best parameters is : {out_models[key].best_params_}\")\n",
    "        print(f\"Best cross validation score is : {out_models[key].best_score_:.8f}\")\n",
    "        print(f\"Best training score is : {out_models[key].score(X_train, y_train):.8f}\")\n",
    "        print(f\"Best testing score is : {out_models[key].score(X_test, y_test):.8f}\")\n",
    "        print('-' * 80)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80b99124-6577-49cc-b22c-101b4fb7579d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Normalizer</th>\n",
       "      <th>Vertorizer_name</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>CV_Score</th>\n",
       "      <th>Training_Score</th>\n",
       "      <th>Testing_Score</th>\n",
       "      <th>Score_Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.903547</td>\n",
       "      <td>0.971375</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.068390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.899815</td>\n",
       "      <td>0.987554</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.077107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.896078</td>\n",
       "      <td>0.981954</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.076481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.892970</td>\n",
       "      <td>0.989421</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>0.071511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.890481</td>\n",
       "      <td>0.986310</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.075862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.889856</td>\n",
       "      <td>0.974487</td>\n",
       "      <td>0.898010</td>\n",
       "      <td>0.076477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>0.876784</td>\n",
       "      <td>0.922215</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.054056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.876170</td>\n",
       "      <td>0.981332</td>\n",
       "      <td>0.890547</td>\n",
       "      <td>0.090784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>0.875538</td>\n",
       "      <td>0.937772</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.079563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>0.872437</td>\n",
       "      <td>0.910392</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.039745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.871185</td>\n",
       "      <td>0.980709</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.107575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>0.862474</td>\n",
       "      <td>0.985688</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.137429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word_Normalizer  Vertorizer_name               Model_Name  CV_Score  \\\n",
       "1   WordNetLemmatizer  CountVectorizer              Naive Bayes  0.903547   \n",
       "4   WordNetLemmatizer  TfidfVectorizer              Naive Bayes  0.899815   \n",
       "7       PorterStemmer  CountVectorizer              Naive Bayes  0.896078   \n",
       "10      PorterStemmer  TfidfVectorizer              Naive Bayes  0.892970   \n",
       "5   WordNetLemmatizer  TfidfVectorizer      Logistic Regression  0.890481   \n",
       "11      PorterStemmer  TfidfVectorizer      Logistic Regression  0.889856   \n",
       "3   WordNetLemmatizer  CountVectorizer  RandomForest Classifier  0.876784   \n",
       "2   WordNetLemmatizer  CountVectorizer      Logistic Regression  0.876170   \n",
       "9       PorterStemmer  CountVectorizer  RandomForest Classifier  0.875538   \n",
       "6   WordNetLemmatizer  TfidfVectorizer  RandomForest Classifier  0.872437   \n",
       "8       PorterStemmer  CountVectorizer      Logistic Regression  0.871185   \n",
       "12      PorterStemmer  TfidfVectorizer  RandomForest Classifier  0.862474   \n",
       "\n",
       "    Training_Score  Testing_Score  Score_Difference  \n",
       "1         0.971375       0.902985          0.068390  \n",
       "4         0.987554       0.910448          0.077107  \n",
       "7         0.981954       0.905473          0.076481  \n",
       "10        0.989421       0.917910          0.071511  \n",
       "5         0.986310       0.910448          0.075862  \n",
       "11        0.974487       0.898010          0.076477  \n",
       "3         0.922215       0.868159          0.054056  \n",
       "2         0.981332       0.890547          0.090784  \n",
       "9         0.937772       0.858209          0.079563  \n",
       "6         0.910392       0.870647          0.039745  \n",
       "8         0.980709       0.873134          0.107575  \n",
       "12        0.985688       0.848259          0.137429  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display model scores\n",
    "model_df['Score_Difference'] = model_df['Training_Score'] - model_df['Testing_Score']\n",
    "model_df.sort_values(by='CV_Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd3f7c-f58d-4b08-8956-f273f1ffab61",
   "metadata": {},
   "source": [
    "#### Analyze models performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffc9f7-82cf-42d0-ab38-e148da71910f",
   "metadata": {},
   "source": [
    "We use `PorterStemmer` for word normalization and `CountVectorizer` for feature extraction with `Naive Bayes` (model #7).\n",
    "- Best parameters is : {'nb__alpha': 0.1, 'vec__min_df': 2, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}\n",
    "\n",
    "While the CV score is nearly identical to the best model #1, the low difference between training and testing scores suggests it is not overfitting, unlike model #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c81bb695-5bf9-4065-a563-dcf7c5e70166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model\n",
    "model = out_models[('CountVectorizer', 'Naive Bayes', 'WordNetLemmatizer')].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd41e7e8-9824-46b5-b60a-5b839cf84c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "data = datas['PorterStemmer']\n",
    "X_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6232518c-0440-4d20-bb83-c282a406877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get classification report\n",
    "score = classification_report(y_test, y_pred, output_dict=True)\n",
    "score_df = pd.DataFrame(score).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f89bcc22-d557-42bb-9bde-70931926823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "wine    0.53\n",
       "beer    0.47\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dispkay baseline\n",
    "y.value_counts(normalize=True).round(2)\n",
    "# classes are well-balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae2d8b05-aec1-4b6a-8b02-1577b3ad12b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beer</th>\n",
       "      <td>0.868293</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.905852</td>\n",
       "      <td>188.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.873832</td>\n",
       "      <td>0.909976</td>\n",
       "      <td>214.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.90796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.908766</td>\n",
       "      <td>0.910320</td>\n",
       "      <td>0.907914</td>\n",
       "      <td>402.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.911383</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.908047</td>\n",
       "      <td>402.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score    support\n",
       "beer           0.868293  0.946809  0.905852  188.00000\n",
       "wine           0.949239  0.873832  0.909976  214.00000\n",
       "accuracy       0.907960  0.907960  0.907960    0.90796\n",
       "macro avg      0.908766  0.910320  0.907914  402.00000\n",
       "weighted avg   0.911383  0.907960  0.908047  402.00000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display classification report\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6bfea-3b93-4c3b-aeb9-1c8948864412",
   "metadata": {},
   "source": [
    "It's a good classification model has high precision, recall, and F1-scores for each class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
