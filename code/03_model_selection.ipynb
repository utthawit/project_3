{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f945e35-698e-4c02-9db3-af9a5208a7ab",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106b514e-5683-4faf-8f4c-675e07f15833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a1085-b3ff-466f-a2b9-2d6dcf7dd106",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a147786e-9227-4dc8-89ae-2a3b1f04ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df = pd.read_csv('../data/subreddit_combine_title_body.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a73ea4-08cb-4f2e-84b5-259c7ea97199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17j7oej</td>\n",
       "      <td>https://www.reddit.com/r/wine/comments/17j7oej...</td>\n",
       "      <td>743</td>\n",
       "      <td>2023-10-30 00:18:37</td>\n",
       "      <td>wine</td>\n",
       "      <td>[Megathread] How much is my wine worth? Is it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1gmbv5t</td>\n",
       "      <td>https://www.reddit.com/r/wine/comments/1gmbv5t...</td>\n",
       "      <td>16</td>\n",
       "      <td>2024-11-08 13:00:27</td>\n",
       "      <td>wine</td>\n",
       "      <td>Free Talk Friday Bottle porn without notes, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1gogepp</td>\n",
       "      <td>https://i.redd.it/6gdvjahxb60e1.jpeg</td>\n",
       "      <td>21</td>\n",
       "      <td>2024-11-11 08:19:05</td>\n",
       "      <td>wine</td>\n",
       "      <td>Started Journey to Master I have great study m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1goebub</td>\n",
       "      <td>https://www.reddit.com/gallery/1goebub</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-11-11 06:36:49</td>\n",
       "      <td>wine</td>\n",
       "      <td>NV Pierre Peters Howdy Winos! Anyone have any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1goj0bf</td>\n",
       "      <td>https://www.reddit.com/r/wine/comments/1goj0bf...</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-11-11 10:37:00</td>\n",
       "      <td>wine</td>\n",
       "      <td>Vouvray Chenin Blanc I mostly drink reds, but ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                url comms_num  \\\n",
       "0  17j7oej  https://www.reddit.com/r/wine/comments/17j7oej...       743   \n",
       "1  1gmbv5t  https://www.reddit.com/r/wine/comments/1gmbv5t...        16   \n",
       "2  1gogepp               https://i.redd.it/6gdvjahxb60e1.jpeg        21   \n",
       "3  1goebub             https://www.reddit.com/gallery/1goebub         7   \n",
       "4  1goj0bf  https://www.reddit.com/r/wine/comments/1goj0bf...        13   \n",
       "\n",
       "               created subreddit  \\\n",
       "0  2023-10-30 00:18:37      wine   \n",
       "1  2024-11-08 13:00:27      wine   \n",
       "2  2024-11-11 08:19:05      wine   \n",
       "3  2024-11-11 06:36:49      wine   \n",
       "4  2024-11-11 10:37:00      wine   \n",
       "\n",
       "                                          title_body  \n",
       "0  [Megathread] How much is my wine worth? Is it ...  \n",
       "1  Free Talk Friday Bottle porn without notes, ra...  \n",
       "2  Started Journey to Master I have great study m...  \n",
       "3  NV Pierre Peters Howdy Winos! Anyone have any ...  \n",
       "4  Vouvray Chenin Blanc I mostly drink reds, but ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd60e7-8a84-40bc-86d9-a0d9c275427e",
   "metadata": {},
   "source": [
    "#### Vertorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ab6039-252d-482b-adb2-db2ad3ba5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined base word helper function\n",
    "lemma_token = WordNetLemmatizer()\n",
    "stem_token = PorterStemmer()\n",
    "\n",
    "def lemma_tokenizer(words):\n",
    "    return ' '.join([lemma_token.lemmatize(w) for w in words.split()])\n",
    "\n",
    "def stem_tokenizer(words):\n",
    "    return ' '.join([stem_token.stem(w) for w in words.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e4a4dc-ad4b-42b7-9fb1-bdf0ade28513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup features\n",
    "X = combine_df['title_body']\n",
    "y = combine_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b53f6d-3650-4706-bbc2-cd13510f0657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "wine    53.16\n",
       "beer    46.84\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find baseline\n",
    "y.value_counts(normalize=True).mul(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186c57c5-ba40-46c5-b0b8-3cebb7352279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split with Lemmatizer\n",
    "X_lemma = X.apply(lemma_tokenizer)\n",
    "X_lemma_train, X_lemma_test, y_lemma_train, y_lemma_test = \\\n",
    "train_test_split(X_lemma, y, stratify=y, random_state=42, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d4d033-02fb-4dfe-831a-9b5d5e4f213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split with Porter stemmer\n",
    "X_stem = X.apply(stem_tokenizer)\n",
    "X_stem_train, X_stem_test, y_stem_train, y_stem_test = \\\n",
    "train_test_split(X_stem, y, stratify=y, random_state=42, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607da808-2b89-47f6-90cf-02e999067f83",
   "metadata": {},
   "source": [
    "#### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b73e09a5-e318-48c2-b34c-5faba71ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined models and parameters\n",
    "\n",
    "vectors = {\n",
    "            'CountVectorizer': {'pipeline': ('vec', CountVectorizer()),\n",
    "                                'grid_params': {'vec__stop_words': [None, 'english'],\n",
    "                                           'vec__min_df': [0.01, 0.05, 0.1, 1],\n",
    "                                          }\n",
    "                               },\n",
    "            'TfidfVectorizer': {'pipeline': ('vec', TfidfVectorizer()),\n",
    "                                'grid_params': {'vec__stop_words': [None, 'english'],\n",
    "                                           'vec__min_df': [0.01, 0.05, 0.1, 1],\n",
    "                                          }\n",
    "                               },\n",
    "}\n",
    "\n",
    "models = {'Naive Bayes': {'pipeline': ('nb',MultinomialNB()),\n",
    "                          'grid_params': {\n",
    "                                          'nb__alpha': [0.001, 0.05, 0.1, 1]\n",
    "                                         }\n",
    "        },\n",
    "        'Logistic Regression': \n",
    "                          {'pipeline': ('lr',LogisticRegression()),\n",
    "                           'grid_params': {\n",
    "                                          'lr__C': [0.001, 0.05, 0.1, 1],\n",
    "                                          'lr__solver': ['liblinear']\n",
    "                                         }\n",
    "        },\n",
    "        'RandomForest Classifier': \n",
    "                          {'pipeline': ('rfc', RandomForestClassifier()),\n",
    "                          'grid_params': {\n",
    "                                          'rfc__min_samples_split': [2 ,4, 8, 10],\n",
    "                                          'rfc__min_samples_leaf': [2 ,4, 8, 10],\n",
    "                                          'rfc__max_depth': [None, 1, 5 ,10, 15]\n",
    "                                         }\n",
    "        },\n",
    "}\n",
    "\n",
    "datas = {\n",
    "    'WordNetLemmatizer': {'X_train': X_lemma_train, 'X_test': X_lemma_test, \n",
    "                          'y_train': y_lemma_train, 'y_test': y_lemma_test},\n",
    "    'PorterStemmer' :    {'X_train': X_stem_train, 'X_test': X_stem_test, \n",
    "                          'y_train': y_stem_train, 'y_test': y_stem_test}    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d68ad1e2-2797-4c72-af6f-9ddc1723bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pipeline and gridsearch params\n",
    "vec_models = {}\n",
    "for vec_name, vector in vectors.items():\n",
    "    for cls_name, model in models.items():\n",
    "        vec_models[(vec_name, cls_name)] = {\n",
    "            'pipeline':   Pipeline([\n",
    "                vector['pipeline'],\n",
    "                model['pipeline'],\n",
    "            ]),\n",
    "            'grid_params': {\n",
    "                **vector['grid_params'],\n",
    "                **model['grid_params'],\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5753ef-0c44-4987-a087-0f4973f36e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Running gridsearch for (CountVectorizer, Naive Bayes, WordNetLemmatizer)\n",
      "Best parameters is : {'nb__alpha': 1, 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.97137212\n",
      "Best training score is : 0.99439950\n",
      "Best testing score is : 0.95522388\n",
      "--------------------------------------------------------------------------------\n",
      "2. Running gridsearch for (CountVectorizer, Logistic Regression, WordNetLemmatizer)\n",
      "Best parameters is : {'lr__C': 1, 'lr__solver': 'liblinear', 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.96328522\n",
      "Best training score is : 1.00000000\n",
      "Best testing score is : 0.95771144\n",
      "--------------------------------------------------------------------------------\n",
      "3. Running gridsearch for (CountVectorizer, RandomForest Classifier, WordNetLemmatizer)\n",
      "Best parameters is : {'rfc__max_depth': None, 'rfc__min_samples_leaf': 2, 'rfc__min_samples_split': 8, 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.95893122\n",
      "Best training score is : 0.98942128\n",
      "Best testing score is : 0.94776119\n",
      "--------------------------------------------------------------------------------\n",
      "4. Running gridsearch for (TfidfVectorizer, Naive Bayes, WordNetLemmatizer)\n",
      "Best parameters is : {'nb__alpha': 1, 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.96017809\n",
      "Best training score is : 0.99688861\n",
      "Best testing score is : 0.95024876\n",
      "--------------------------------------------------------------------------------\n",
      "5. Running gridsearch for (TfidfVectorizer, Logistic Regression, WordNetLemmatizer)\n",
      "Best parameters is : {'lr__C': 1, 'lr__solver': 'liblinear', 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.96639855\n",
      "Best training score is : 0.99937772\n",
      "Best testing score is : 0.96766169\n",
      "--------------------------------------------------------------------------------\n",
      "6. Running gridsearch for (TfidfVectorizer, RandomForest Classifier, WordNetLemmatizer)\n",
      "Best parameters is : {'rfc__max_depth': None, 'rfc__min_samples_leaf': 2, 'rfc__min_samples_split': 10, 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.95955001\n",
      "Best training score is : 0.99502178\n",
      "Best testing score is : 0.95273632\n",
      "--------------------------------------------------------------------------------\n",
      "7. Running gridsearch for (CountVectorizer, Naive Bayes, PorterStemmer)\n",
      "Best parameters is : {'nb__alpha': 1, 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.97324089\n",
      "Best training score is : 0.99439950\n",
      "Best testing score is : 0.96766169\n",
      "--------------------------------------------------------------------------------\n",
      "8. Running gridsearch for (CountVectorizer, Logistic Regression, PorterStemmer)\n",
      "Best parameters is : {'lr__C': 1, 'lr__solver': 'liblinear', 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.95768511\n",
      "Best training score is : 1.00000000\n",
      "Best testing score is : 0.95522388\n",
      "--------------------------------------------------------------------------------\n",
      "9. Running gridsearch for (CountVectorizer, RandomForest Classifier, PorterStemmer)\n",
      "Best parameters is : {'rfc__max_depth': 15, 'rfc__min_samples_leaf': 2, 'rfc__min_samples_split': 2, 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.95644210\n",
      "Best training score is : 0.97386434\n",
      "Best testing score is : 0.93034826\n",
      "--------------------------------------------------------------------------------\n",
      "10. Running gridsearch for (TfidfVectorizer, Naive Bayes, PorterStemmer)\n",
      "Best parameters is : {'nb__alpha': 1, 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.95955775\n",
      "Best training score is : 0.99688861\n",
      "Best testing score is : 0.95024876\n",
      "--------------------------------------------------------------------------------\n",
      "11. Running gridsearch for (TfidfVectorizer, Logistic Regression, PorterStemmer)\n",
      "Best parameters is : {'lr__C': 1, 'lr__solver': 'liblinear', 'vec__min_df': 1, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.96515322\n",
      "Best training score is : 1.00000000\n",
      "Best testing score is : 0.96766169\n",
      "--------------------------------------------------------------------------------\n",
      "12. Running gridsearch for (TfidfVectorizer, RandomForest Classifier, PorterStemmer)\n",
      "Best parameters is : {'rfc__max_depth': None, 'rfc__min_samples_leaf': 2, 'rfc__min_samples_split': 4, 'vec__min_df': 0.01, 'vec__stop_words': 'english'}\n",
      "Best cross validation score is : 0.95395222\n",
      "Best training score is : 0.99813317\n",
      "Best testing score is : 0.93781095\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "out_models = {}\n",
    "i = 1\n",
    "# initialize DataFrame to store score output \n",
    "model_df = pd.DataFrame(columns=['Word_Normalizer', 'Vertorizer_name', 'Model_Name', 'CV_Score', 'Training_Score', 'Testing_Score'])\n",
    "for group_name, data in datas.items():\n",
    "    X_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n",
    "    for name, model in vec_models.items():\n",
    "        vec_name, cls_name = name\n",
    "        key = (vec_name, cls_name, group_name)\n",
    "        print(f\"{i}. Running gridsearch for ({vec_name}, {cls_name}, {group_name})\")\n",
    "        out_models[key] = GridSearchCV(estimator=model['pipeline'], param_grid=model['grid_params'], \\\n",
    "                            scoring='accuracy', n_jobs=10, cv=2)\n",
    "        out_models[key].fit(X_train, y_train)\n",
    "        \n",
    "        # add store for model\n",
    "        model_df.loc[i] = [group_name, vec_name, cls_name, out_models[key].best_score_, \n",
    "                            out_models[key].score(X_train, y_train), out_models[key].score(X_test, y_test)]\n",
    "        \n",
    "        # display model result\n",
    "        print(f\"Best parameters is : {out_models[key].best_params_}\")\n",
    "        print(f\"Best cross validation score is : {out_models[key].best_score_:.8f}\")\n",
    "        print(f\"Best training score is : {out_models[key].score(X_train, y_train):.8f}\")\n",
    "        print(f\"Best testing score is : {out_models[key].score(X_test, y_test):.8f}\")\n",
    "        print('-' * 80)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80b99124-6577-49cc-b22c-101b4fb7579d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Normalizer</th>\n",
       "      <th>Vertorizer_name</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>CV_Score</th>\n",
       "      <th>Training_Score</th>\n",
       "      <th>Testing_Score</th>\n",
       "      <th>Score_Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.973241</td>\n",
       "      <td>0.994400</td>\n",
       "      <td>0.967662</td>\n",
       "      <td>0.026738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.971372</td>\n",
       "      <td>0.994400</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.039176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.966399</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.967662</td>\n",
       "      <td>0.031716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.965153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967662</td>\n",
       "      <td>0.032338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.963285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957711</td>\n",
       "      <td>0.042289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.960178</td>\n",
       "      <td>0.996889</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.046640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.959558</td>\n",
       "      <td>0.996889</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.046640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>0.959550</td>\n",
       "      <td>0.995022</td>\n",
       "      <td>0.952736</td>\n",
       "      <td>0.042285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WordNetLemmatizer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>0.958931</td>\n",
       "      <td>0.989421</td>\n",
       "      <td>0.947761</td>\n",
       "      <td>0.041660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.957685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.044776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>0.956442</td>\n",
       "      <td>0.973864</td>\n",
       "      <td>0.930348</td>\n",
       "      <td>0.043516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PorterStemmer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>0.953952</td>\n",
       "      <td>0.998133</td>\n",
       "      <td>0.937811</td>\n",
       "      <td>0.060322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word_Normalizer  Vertorizer_name               Model_Name  CV_Score  \\\n",
       "7       PorterStemmer  CountVectorizer              Naive Bayes  0.973241   \n",
       "1   WordNetLemmatizer  CountVectorizer              Naive Bayes  0.971372   \n",
       "5   WordNetLemmatizer  TfidfVectorizer      Logistic Regression  0.966399   \n",
       "11      PorterStemmer  TfidfVectorizer      Logistic Regression  0.965153   \n",
       "2   WordNetLemmatizer  CountVectorizer      Logistic Regression  0.963285   \n",
       "4   WordNetLemmatizer  TfidfVectorizer              Naive Bayes  0.960178   \n",
       "10      PorterStemmer  TfidfVectorizer              Naive Bayes  0.959558   \n",
       "6   WordNetLemmatizer  TfidfVectorizer  RandomForest Classifier  0.959550   \n",
       "3   WordNetLemmatizer  CountVectorizer  RandomForest Classifier  0.958931   \n",
       "8       PorterStemmer  CountVectorizer      Logistic Regression  0.957685   \n",
       "9       PorterStemmer  CountVectorizer  RandomForest Classifier  0.956442   \n",
       "12      PorterStemmer  TfidfVectorizer  RandomForest Classifier  0.953952   \n",
       "\n",
       "    Training_Score  Testing_Score  Score_Difference  \n",
       "7         0.994400       0.967662          0.026738  \n",
       "1         0.994400       0.955224          0.039176  \n",
       "5         0.999378       0.967662          0.031716  \n",
       "11        1.000000       0.967662          0.032338  \n",
       "2         1.000000       0.957711          0.042289  \n",
       "4         0.996889       0.950249          0.046640  \n",
       "10        0.996889       0.950249          0.046640  \n",
       "6         0.995022       0.952736          0.042285  \n",
       "3         0.989421       0.947761          0.041660  \n",
       "8         1.000000       0.955224          0.044776  \n",
       "9         0.973864       0.930348          0.043516  \n",
       "12        0.998133       0.937811          0.060322  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df['Score_Difference'] = model_df['Training_Score'] - model_df['Testing_Score']\n",
    "model_df.sort_values(by='CV_Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd3f7c-f58d-4b08-8956-f273f1ffab61",
   "metadata": {},
   "source": [
    "#### Analyze models performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffc9f7-82cf-42d0-ab38-e148da71910f",
   "metadata": {},
   "source": [
    "We use `PorterStemmer` for word normalization and `CountVectorizer` for feature extraction with `Naive Bayes` (model #7).\n",
    "- Best parameters is : {'nb__alpha': 1, 'vec__max_df': 1000, 'vec__min_df': 5, 'vec__stop_words': None}\n",
    "\n",
    "While the CV score is nearly identical to the best model #1, the low difference between training and testing scores suggests it is not overfitting, unlike model #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c81bb695-5bf9-4065-a563-dcf7c5e70166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model\n",
    "model = out_models[('CountVectorizer', 'Naive Bayes', 'PorterStemmer')].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd41e7e8-9824-46b5-b60a-5b839cf84c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "data = datas['PorterStemmer']\n",
    "X_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6232518c-0440-4d20-bb83-c282a406877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get classification report\n",
    "score = classification_report(y_test, y_pred, output_dict=True)\n",
    "score_df = pd.DataFrame(score).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f89bcc22-d557-42bb-9bde-70931926823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "wine    0.53\n",
       "beer    0.47\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dispkay baseline\n",
    "y.value_counts(normalize=True).round(2)\n",
    "# classes are well-balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae2d8b05-aec1-4b6a-8b02-1577b3ad12b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beer</th>\n",
       "      <td>0.953368</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.965879</td>\n",
       "      <td>188.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.980861</td>\n",
       "      <td>0.957944</td>\n",
       "      <td>0.969267</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.967662</td>\n",
       "      <td>0.967662</td>\n",
       "      <td>0.967662</td>\n",
       "      <td>0.967662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.967115</td>\n",
       "      <td>0.968334</td>\n",
       "      <td>0.967573</td>\n",
       "      <td>402.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.968004</td>\n",
       "      <td>0.967662</td>\n",
       "      <td>0.967683</td>\n",
       "      <td>402.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "beer           0.953368  0.978723  0.965879  188.000000\n",
       "wine           0.980861  0.957944  0.969267  214.000000\n",
       "accuracy       0.967662  0.967662  0.967662    0.967662\n",
       "macro avg      0.967115  0.968334  0.967573  402.000000\n",
       "weighted avg   0.968004  0.967662  0.967683  402.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display classification report\n",
    "score_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
